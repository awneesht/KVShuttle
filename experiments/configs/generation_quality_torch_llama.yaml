# Llama-3.1-8B only â€” requires A100 (40GB) for FP16
experiment:
  name: generation_quality_fp16
  description: "Llama-3.1-8B FP16 on A100 GPU"

backend: torch

models:
  - llama-3.1-8b

compressors:
  - identity
  - uniform_int8
  - uniform_int4
  - kivi_2bit
  - cachegen
  - palu_lr
  - cascade_prune50_int4

bandwidths_gbps:
  - 10

prompts:
  source: wikitext
  count: 50
  min_tokens: 128
  max_tokens: 512

evaluation:
  attention_error: true
  perplexity: true
  token_agreement: true

output:
  dir: experiments/results/generation_quality_fp16_llama
  save_per_layer: false
