{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KVShuttle: FP16 Generation Quality on GPU\n",
    "\n",
    "## Setup\n",
    "1. Create zip locally: `cd KVShuttle && zip -r kvshuttle.zip . -x '.git/*'`\n",
    "2. Set runtime to GPU (T4 or A100)\n",
    "3. Run all cells — upload zip when prompted\n",
    "4. For Llama: paste your HF token and accept license first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    raise RuntimeError(\"No GPU detected! Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies + upload KVShuttle\n",
    "!pip install -q transformers accelerate datasets pyyaml tqdm\n",
    "\n",
    "import os\n",
    "if not os.path.exists(\"kvshuttle\"):\n",
    "    from google.colab import files\n",
    "    print(\"Upload kvshuttle.zip\")\n",
    "    uploaded = files.upload()\n",
    "    !unzip -qo kvshuttle.zip -d KVShuttle\n",
    "    %cd KVShuttle\n",
    "    !pip install -q -e .\n",
    "else:\n",
    "    print(\"KVShuttle already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace login (required for gated models like Llama)\n",
    "# Paste your token from https://huggingface.co/settings/tokens\n",
    "from huggingface_hub import login\n",
    "login(token=\"PASTE_YOUR_HF_TOKEN_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "from kvshuttle.models.loader_torch import TORCH_MODEL_REGISTRY\n",
    "from kvshuttle.compression.registry import list_compressors\n",
    "print(f\"Models: {list(TORCH_MODEL_REGISTRY.keys())}\")\n",
    "print(f\"Compressors: {list_compressors()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Write config inline (avoids zip caching issues)\nimport yaml\nfrom pathlib import Path\n\nconfig = {\n    \"experiment\": {\"name\": \"generation_quality_fp16\", \"description\": \"Mistral-7B + Qwen2.5-7B FP16 on A100 GPU\"},\n    \"backend\": \"torch\",\n    \"models\": [\"mistral-7b\", \"qwen2.5-7b\"],\n    \"compressors\": [\"identity\", \"uniform_int8\", \"uniform_int4\", \"kivi_2bit\", \"cachegen\", \"palu_lr\", \"cascade_prune50_int4\"],\n    \"bandwidths_gbps\": [10],\n    \"prompts\": {\"source\": \"wikitext\", \"count\": 50, \"min_tokens\": 128, \"max_tokens\": 512},\n    \"evaluation\": {\"attention_error\": True, \"perplexity\": True, \"token_agreement\": True},\n    \"output\": {\"dir\": \"experiments/results/generation_quality_fp16_7b\", \"save_per_layer\": False},\n}\n\nconfig_path = Path(\"experiments/configs/generation_quality_torch_7b.yaml\")\nconfig_path.parent.mkdir(parents=True, exist_ok=True)\nwith open(config_path, \"w\") as f:\n    yaml.dump(config, f)\nprint(f\"Wrote config to {config_path}\")\n\n!python -m experiments.scripts.run_experiment {config_path}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inspect results\nimport json\nfrom pathlib import Path\n\nresults_dirs = [\n    Path(\"experiments/results/generation_quality_fp16_7b\"),\n    Path(\"experiments/results/generation_quality_fp16_llama\"),\n    Path(\"experiments/results/generation_quality_fp16_smoke\"),\n    Path(\"experiments/results/generation_quality_fp16\"),\n]\nresults_path = next((d / \"results.json\" for d in results_dirs if (d / \"results.json\").exists()), None)\n\nif results_path:\n    with open(results_path) as f:\n        data = json.load(f)\n    print(f\"Results: {len(data['results'])} from {results_path}\")\n\n    import pandas as pd\n    df = pd.DataFrame(data['results'])\n    agg_cols = {c: 'mean' for c in ['mean_key_cosine_sim', 'perplexity_delta', 'token_agreement'] if c in df.columns}\n    if agg_cols:\n        display(df.groupby(['model', 'compressor']).agg(agg_cols).round(4))\n    else:\n        print(\"No quality metrics — check WARNING logs above\")\nelse:\n    print(\"Results not found. Check experiment output above for errors.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "if results_path and results_path.exists():\n",
    "    from google.colab import files\n",
    "    files.download(str(results_path))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}