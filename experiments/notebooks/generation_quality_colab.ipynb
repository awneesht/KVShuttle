{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# KVShuttle: FP16 Generation Quality on GPU\n",
        "\n",
        "This notebook runs the end-to-end generation quality experiment using\n",
        "FP16 PyTorch models on a CUDA GPU (T4/A100).\n",
        "\n",
        "**What it does:**\n",
        "1. Installs KVShuttle and dependencies\n",
        "2. Runs FP16 model inference with 7 compressors on 3 models\n",
        "3. Measures attention error, perplexity delta, and token agreement\n",
        "4. Saves results JSON for local figure generation\n",
        "\n",
        "**Runtime:** ~2-3 hours on T4 (3 models x 50 prompts x 7 compressors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
        "else:\n",
        "    raise RuntimeError(\"No GPU detected! Go to Runtime > Change runtime type > GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers accelerate datasets pyyaml tqdm\n",
        "\n",
        "# Clone and install KVShuttle\n",
        "!git clone https://github.com/your-repo/KVShuttle.git 2>/dev/null || echo \"Already cloned\"\n",
        "%cd KVShuttle\n",
        "!pip install -q -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify KVShuttle installation and torch backend\n",
        "from kvshuttle.models.loader_torch import TORCH_MODEL_REGISTRY, load_model_torch\n",
        "from kvshuttle.models.kv_extractor_torch import extract_kv_cache_torch\n",
        "from kvshuttle.models.kv_injector_torch import forward_continuation_with_kv_cache_torch\n",
        "from kvshuttle.compression.registry import list_compressors\n",
        "\n",
        "print(f\"Available models: {list(TORCH_MODEL_REGISTRY.keys())}\")\n",
        "print(f\"Available compressors: {list_compressors()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the experiment\n",
        "\n",
        "Uses the `generation_quality_torch.yaml` config with `backend: torch`.\n",
        "For a quick test, reduce `prompts.count` in the config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the full experiment\n",
        "!python -m experiments.scripts.run_experiment experiments/configs/generation_quality_torch.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect results\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "results_path = Path(\"experiments/results/generation_quality_fp16/results.json\")\n",
        "if results_path.exists():\n",
        "    with open(results_path) as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"Metadata: {json.dumps(data['metadata'], indent=2)}\")\n",
        "    print(f\"\\nTotal results: {len(data['results'])}\")\n",
        "    \n",
        "    # Quick summary\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(data['results'])\n",
        "    summary = df.groupby(['model', 'compressor']).agg({\n",
        "        'mean_key_cosine_sim': 'mean',\n",
        "        'perplexity_delta': 'mean',\n",
        "        'token_agreement': 'mean',\n",
        "    }).round(4)\n",
        "    display(summary)\n",
        "else:\n",
        "    print(\"Results not found. Check experiment output above for errors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download results for local figure generation\n",
        "if results_path.exists():\n",
        "    from google.colab import files\n",
        "    files.download(str(results_path))\n",
        "    print(\"Downloaded results.json\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
